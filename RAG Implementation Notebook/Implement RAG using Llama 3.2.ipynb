{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd20d6a",
   "metadata": {},
   "source": [
    "\n",
    "## Building a Conversational Chatbot in your local with Llama 3.2 using Ollama\n",
    "\n",
    "This notebook demonstrates a streamlined workflow for building a **Retrieval-Augmented Generation (RAG)** system locally using **Llama 3.2** via **Ollama**. The goal is to enable efficient document-based question answering by integrating document ingestion, vector database storage, and a conversational retrieval system. \n",
    "\n",
    "The following steps are covered:\n",
    "\n",
    "1. **Document Preparation and Splitting**: PDF documents from a specified directory are loaded, and text passages are split into manageable chunks using `RecursiveCharacterTextSplitter`. This ensures that documents are split based on size and overlap to optimize retrieval accuracy.\n",
    "\n",
    "2. **Ingesting Documents into Vector Database**: The split documents are embedded using `HuggingFaceEmbeddings` and stored in a local FAISS vector database, facilitating fast and scalable document retrieval.\n",
    "\n",
    "3. **Building the Conversation Chain**: A conversational chain is built using **Llama 3.2** to retrieve relevant information based on user queries. This chain ensures that the responses are accurate, concise, and relevant to the context of the chat history, while managing session history to maintain the flow of conversation.\n",
    "\n",
    "4. **Similarity Score Calculation**: To evaluate the relevancy of the system’s responses, the notebook includes a function to calculate the similarity score between the generated answers and the source documents using `SentenceTransformer`.\n",
    "\n",
    "The notebook is designed for local execution, leveraging the performance and capabilities of Llama 3.2 via the **Ollama** API to offer a robust and efficient Q&A solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e379a0a",
   "metadata": {},
   "source": [
    "Below cell imports the required libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d076e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "install_packages=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc305674",
   "metadata": {},
   "outputs": [],
   "source": [
    "if install_packages==True:\n",
    "    !pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c4177b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikrambhat/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#from htmlTemplate import css, bot_template, user_template\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c264d9e",
   "metadata": {},
   "source": [
    "### Enter your pdf file name below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158eb08",
   "metadata": {},
   "source": [
    "### Step 1: Prepare documents and their metadata\n",
    "The `prepare_and_split_docs` function loads PDF documents from a specified directory using `DirectoryLoader`. It then splits the loaded documents into smaller text chunks using `RecursiveCharacterTextSplitter` with a chunk size of 512 and an overlap of 256. The splitter ensures that document metadata is preserved and splits content based on specified separators like new lines and spaces. The function finally returns the split documents and prints the total number of text passages created. This process aids in efficient document handling for downstream tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e49bddd35c6d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T08:05:12.765779Z",
     "start_time": "2024-01-07T08:05:12.763477Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_and_split_docs(pdf_directory):\n",
    "    # Load the documents\n",
    "    loader = DirectoryLoader(pdf_directory, glob=\"**/*.pdf\", show_progress=True, loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,  # Use the smaller chunk size here to avoid repeating splitting logic\n",
    "        chunk_overlap=256,\n",
    "        disallowed_special=(),\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    "    )\n",
    "\n",
    "    # Split the documents and keep metadata\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f576d0",
   "metadata": {},
   "source": [
    "### Step 3: Ingest into Vector Database locally\n",
    "\n",
    "The `ingest_into_vectordb` function is designed for processing and indexing a collection of documents into a vector database using FAISS (Facebook AI Similarity Search) for efficient similarity searches. It operates as follows:\n",
    "\n",
    "1. **Embedding Creation**: It generates embeddings for the input documents (`split_docs`) using the Hugging Face model `'sentence-transformers/all-MiniLM-L6-v2'`. This model is specifically chosen for its efficiency in creating sentence-level embeddings and is set to run on the CPU.\n",
    "\n",
    "2. **Vector Database Indexing**: Utilizes the generated embeddings to create a FAISS vector database. FAISS is used for its ability to efficiently handle large-scale similarity searches and clustering of dense vectors.\n",
    "\n",
    "3. **Local Storage**: After creating the vector database, the function saves it locally to the path specified by `DB_FAISS_PATH`, ensuring the data can be easily accessed for future similarity searches or retrieval tasks.\n",
    "\n",
    "The primary purpose of this function is to transform textual data into a structured, searchable vector format, facilitating efficient and scalable retrieval tasks such as document similarity searches or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba2d9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_into_vectordb(split_docs):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
    "    db = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    print(\"Documents are inserted into FAISS vectorstore\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ba4cd",
   "metadata": {},
   "source": [
    "### Step 4: Set up Conversation Chain using LLM\n",
    "The `get_conversation_chain(retriever)` function creates a stateful conversational RAG system.\n",
    "\n",
    "1. It initializes the `llama3.2` model and defines two prompts:\n",
    "   - A contextualization prompt to handle the user's query in light of the chat history.\n",
    "   - A system prompt for answering concisely with 2-3 sentences based on retrieved documents.\n",
    "\n",
    "2. It builds a `history_aware_retriever` using the retriever, LLM, and the contextualization prompt to ensure responses are context-aware.\n",
    "\n",
    "3. A `question_answer_chain` is set up to respond with answers limited to 50 words.\n",
    "\n",
    "4. These components are combined into a RAG chain using `create_retrieval_chain`.\n",
    "\n",
    "5. To manage chat history across sessions, it defines `get_session_history`, which stores and retrieves message history by session ID.\n",
    "\n",
    "6. Finally, a `RunnableWithMessageHistory` integrates the RAG chain with chat history management, ensuring the bot maintains state and provides contextually relevant responses throughout the conversation.\n",
    "\n",
    "This function sets up a sophisticated conversational AI system combining the LLaMA model for language generation and a vector database for information retrieval, enhanced with a callback manager for additional processing and a conversation memory buffer for context management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daeb1adc421d294e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:11.383224Z",
     "start_time": "2024-01-07T07:48:11.380239Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_conversation_chain(retriever):\n",
    "    llm = Ollama(model=\"llama3.2\")\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given the chat history and the latest user question, \"\n",
    "        \"provide a response that directly addresses the user's query based on the provided  documents. \"\n",
    "        \"Do not rephrase the question or ask follow-up questions.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "    ### Answer question ###\n",
    "    system_prompt = (\n",
    "        \"As a personal chat assistant, provide accurate and relevant information based on the provided document in 2-3 sentences. \"\n",
    "        \"Answe should be limited to 50 words and 2-3 sentences.  do not prompt to select answers or do not formualate a stand alone question. do not ask questions in the response. \"\n",
    "        \"{context}\"\n",
    "    )\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "    ### Statefully manage chat history ###\n",
    "    store = {}\n",
    "\n",
    "\n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    print(\"Conversational chain created\")\n",
    "    return conversational_rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e69dfd",
   "metadata": {},
   "source": [
    "### Step 5: Calculate Document Similarity in the LLMs Response\n",
    "The `calculate_similarity_score` function computes the cosine similarity between a given answer and a list of context documents using Sentence Transformers. It first encodes the answer and context documents into embeddings. Then, it calculates the cosine similarities between the answer embedding and the context embeddings. The function returns the maximum similarity score, indicating how closely the answer relates to the most relevant context document. Scores range from 0 (no similarity) to 1 (perfect similarity), with higher scores reflecting better alignment with the context.\n",
    "\n",
    "Essentially, this function serves as a mechanism to check the alignment of the chatbot's response with the information in the source documents, ensuring the response's accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d0cc0f87a9595c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:35.893137Z",
     "start_time": "2024-01-07T07:48:35.887077Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_similarity_score(answer: str, context_docs: list) -> float:\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    context_docs = [doc.page_content for doc in context_docs]\n",
    "    # Encode the answer and context documents\n",
    "    answer_embedding = model.encode(answer, convert_to_tensor=True)\n",
    "    context_embeddings = model.encode(context_docs, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = util.pytorch_cos_sim(answer_embedding, context_embeddings)\n",
    "\n",
    "    # Return the maximum similarity score from the context documents\n",
    "    max_score = similarities.max().item() \n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38df5c",
   "metadata": {},
   "source": [
    "Now that we have crafted all the necessary functions, it's time to put them into action and test their functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e435e003cfe91c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:11.041141Z",
     "start_time": "2024-01-07T10:24:10.938343Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are split into 3 passages\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikrambhat/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are inserted into FAISS vectorstore\n",
      "Conversational chain created\n"
     ]
    }
   ],
   "source": [
    "pdf_directory=\"data_directory\"\n",
    "split_docs=prepare_and_split_docs(pdf_directory)\n",
    "vector_db= ingest_into_vectordb(split_docs)\n",
    "\n",
    "\n",
    "retriever =vector_db.as_retriever()\n",
    "conversational_rag_chain=get_conversation_chain(retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df276c10",
   "metadata": {},
   "source": [
    "### Ask your Question\n",
    "\n",
    "We created a conversational chain and now ready to chat with your own data. \n",
    "\n",
    "\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e513bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question=\"Can you summarise the documents ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0c000474595b40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:54.014449Z",
     "start_time": "2024-01-07T10:44:50.322823Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vikram Bhat has 8 years of experience in data science technologies with expertise in retrieving augmented generation (RAG) systems, machine learning, and data visualization. He holds an MSc in Data Science and Analytics from University College Cork, Ireland, and has a strong background in programming languages like Python, R, and SQL.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa1=conversational_rag_chain.invoke(\n",
    "    {\"input\": user_question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    }\n",
    ")\n",
    "print(qa1[\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c22c53",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "We have now received an answer for a provided question. We can also view the conversation history and source documents in the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be79e26f",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversation Chain\n",
      "{'input': 'Can you summarise the documents ?', 'chat_history': [], 'context': [Document(metadata={'source': 'data_directory/Vikram Bhat - Experienced Data Scientist.pdf', 'page': 1}, page_content='WatsonX LLMS platform, focusing on enhancing language processing and information retrieval capabilities. ● Employed Elasticsearch for the vector storage solution, significantly enhancing data indexing and search response efficiency, which resulted in faster and more accurate query handling. ● Developed models for Regression, Prediction, Clustering, and Time-Series analysis, catering to sectors like Wealth Management, Utilities, and Healthcare by delving into specific industry challenges. ● Managed comprehensive datasets, ensuring meticulous preparation, and cleansing to construct and operationalize machine learning models within Watson Studio, facilitating real-time scoring in Python via Jupyter notebooks. ● Created interactive R-shiny dashboards for dynamic visualization and evaluation of machine learning models, utilizing an array of libraries like ggplot, flexdashboard, shinydashboard, and plotly for dashboard construction. ● Formulated a seamless pipeline from inception to deployment for machine learning models within Watson Studio, with deployment on AWS Sagemaker and subsequent monitoring using IBM Watson OpenScale. ● Demonstrated proficiency in articulating complex results to both internal and external stakeholders, ensuring clear and effective communication.  Data Analyst /Voxpro Groups, Cork      JAN 2018 – APR 2019 ● Identify, extract, and transform both structured and unstructured data collected from internal tools and heterogeneous systems. Build SQL scripts to load the data into data warehouse. ● Creation of models, reports and visualization solutions providing interpretation and insight into business data and processes. Used python libraries and tools such as Power BI to provide visualization.  BI Developer /Cognizant Technology Solutions, Bengaluru    JUL 12 – AUG 16 ● Managed data extraction from various sources including Mainframes and SQL Servers, and performed ETL operations with Informatica tools. Developed comprehensive data mappings in a Hadoop environment to process and prepare data for database integration.'), Document(metadata={'source': 'data_directory/Vikram Bhat - Experienced Data Scientist.pdf', 'page': 1}, page_content='2 Work Experience ● 8 years of experience in design, development, and management of Data science technologies. ● Advanced proficiency in developing Retrieval-Augmented Generation (RAG) systems using WatsonX LLMS, enhancing the synergy between language model processing and search functionality through innovative use of Langchain and Elasticsearch. ● Proficient in analyzing the problems and data cleaning, build, deploy and score machine learning models using Python and R. ● Use sklearn, Pandas, numpy, tensorflow, wml and other libraries to build and deploy Regression, Prediction and Clustering models. ● Build and deploy end to end ML pipeline using IBM Watson Studio and AWS Sagemaker environments. ● Expert in building data reporting/visualization using matplotlib, seaborn, plotly in Python and ggplot, flexdashboard, shinydashboard, plotly, leaflet in R and reporting tools such as Tableu, Cognos Analytics etc.  Data Scientist /IBM, Dublin           MAY 2019 –  ● Integrated complex language models using Langchain for improved processing efficiency and system functionality in natural language tasks. ● Led the creation and implementation of advanced Retrieval-Augmented Generation systems on the WatsonX LLMS platform, focusing on enhancing language processing and information retrieval capabilities. ● Employed Elasticsearch for the vector storage solution, significantly enhancing data indexing and search response efficiency, which resulted in faster and more accurate query handling. ● Developed models for Regression, Prediction, Clustering, and Time-Series analysis, catering to sectors like Wealth Management, Utilities, and Healthcare by delving into specific industry challenges. ● Managed comprehensive datasets, ensuring meticulous preparation, and cleansing to construct and operationalize machine learning models within Watson Studio, facilitating real-time scoring in Python via Jupyter notebooks. ● Created interactive R-shiny dashboards for dynamic visualization and evaluation of machine learning models, utilizing an array of libraries like ggplot, flexdashboard, shinydashboard, and plotly for dashboard construction. ● Formulated a seamless pipeline from inception to deployment for machine learning models within Watson Studio, with deployment on AWS Sagemaker and subsequent monitoring using IBM Watson OpenScale. ● Demonstrated proficiency in articulating complex results to both internal and external stakeholders, ensuring clear and effective communication.  Data Analyst /Voxpro Groups, Cork      JAN 2018 – APR 2019 ● Identify, extract, and transform both structured and'), Document(metadata={'source': 'data_directory/Vikram Bhat - Experienced Data Scientist.pdf', 'page': 0}, page_content='Vikram Bhat  Dublin, Ireland  +353-894410686  Vikrambhat249@gmail.com   Data scientist with deep expertise in data science and analytics, strong hold on Mathematics and Statistics, fascination in programming and comprehensive experience in handling large language models and building RAG systems. I am seeking an opportunity with a growing organization wherein there is a chance to apply my analytical and technical skills acquired.  Skills ● Retrieval Augmented Generation ● LLM ● Data Science  ● Machine learning ● Data mining/Processing ● Data warehouse  ● Python ● R  ● SQL ● Shell Scripting ● Statistics ● Deep Learning ● Data Visualization ● AWS Sagemaker ● Problem Solving ● Critical Thinking ● Customer Service ● Creative Thinking   Education MSc in Data science and analytics/University College Cork, Cork.                GPA 1:1 OCTOBER 2017 Data Mining and Machine Learning, Multivariate Methods for Data Analysis, Linear Regression and Generalized Linear Modelling, Programming in Python with Data Science Applications, Information retrieval systems, Text mining and sentiment analysis. PROJECTS MAXIMIZE STUDENTS’ EXPERIENCE IN A UNIVERSITY USING ARTIFICIAL NEURAL NETWORKS AND SUPPORT VECTOR MACHINES      MAY 2017 – SEP 2017  Scholarship: Boole / Presidential Merit Scholarships for top International(non-EU) student 2016/2017.  B.E. in Computer Science/Sir M.VIT, Bengaluru GPA 1:1 JUNE 2012 Relational Database, C programming, Compiler Design, Data Structures, Advanced Mathematics and Statistics.')], 'answer': 'Vikram Bhat has 8 years of experience in data science technologies with expertise in retrieving augmented generation (RAG) systems, machine learning, and data visualization. He holds an MSc in Data Science and Analytics from University College Cork, Ireland, and has a strong background in programming languages like Python, R, and SQL.'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Conversation Chain\")\n",
    "print(qa1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d31975",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "source": [
    "### Calculate Similarity score between the LLM Response and context documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "acfdf2f4",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vikrambhat/Documents/GitHub/RAG-Implementation-with-ConversationUI/RAG Implementation Notebook/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Similarity Score: 0.53\n"
     ]
    }
   ],
   "source": [
    "\n",
    "answer = qa1[\"answer\"]\n",
    "context_docs = qa1[\"context\"]\n",
    "similarity_score = calculate_similarity_score(answer, context_docs)\n",
    "\n",
    "print(\"Context Similarity Score:\", round(similarity_score,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2018700c",
   "metadata": {},
   "source": [
    "### Sumamry\n",
    "This notebook demonstrates the implementation of a Retrieval-Augmented Generation (RAG) pipeline using Llama 3.2 via the Ollama API. We began by preparing and splitting PDF documents into manageable chunks, then ingested these into a vector database with FAISS and Hugging Face embeddings for efficient retrieval.\n",
    "\n",
    "We integrated the retriever with a conversation chain driven by an LLM, using customized system prompts to generate concise responses based on relevant documents. Additionally, we implemented a method for managing conversation history to maintain context in multi-turn interactions, along with calculating similarity scores between generated answers and source documents using SentenceTransformers.\n",
    "\n",
    "Overall, this notebook serves as a guide for creating a locally deployable RAG application, effectively combining Llama 3.2 for inference and FAISS for document retrieval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
