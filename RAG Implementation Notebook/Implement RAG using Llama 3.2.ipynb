{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbd20d6a",
   "metadata": {},
   "source": [
    "\n",
    "## Building a Conversational Chatbot with your data using Ollama  \n",
    "\n",
    "This notebook guides you through the creation of a chatbot tailored to your specific data needs. Utilizing HuggingFaceEmbeddings and FAISS, the project transforms documents into vectors for a local vector storage system. Then it integrates the \"meta-llama/llama-2-7b-chat\" model from your local machine. The `langchain` library plays a crucial role in this process, aiding in tasks like chunking documents, indexing data in vector db, managing conversation chains with memory buffers, and crafting prompt templates.\n",
    "\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "- **PDF Content Processing**: When users upload PDF files, the notebook extracts the text, segments it into manageable chunks, and indexes these chunks in in a vector db locally using HuggingFaceEmbeddings and FAISS.\n",
    "- **Data-Driven Query Handling**: Users can pose questions to the chatbot, which searches the indexed data for relevant answers.\n",
    "- **Integrating Vector Database and LLMs**: We leverage `langchain`'s capabilities to link vector database indexing with llama-2 LLMs, enabling a seamless conversational experience with memory and retrieval functionalities.\n",
    "- **Hallucination Check**: The notebook includes a mechanism to detect and correct any hallucinations or inaccuracies in the LLM's responses.\n",
    "\n",
    "### Prerequisites for Running the Notebook:\n",
    "\n",
    "\n",
    "1. **Library Requirements**: Confirm that you have installed all libraries specified in the `requirements (local rag).txt` file by `pip install -r requirements (local rag).txt`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e379a0a",
   "metadata": {},
   "source": [
    "Below cell imports the required libraries to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c4177b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PyPDF2\n",
    "from langchain.document_loaders import DirectoryLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # import hf embedding\n",
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c264d9e",
   "metadata": {},
   "source": [
    "### Enter your pdf file name below\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7158eb08",
   "metadata": {},
   "source": [
    "### Step 1: Prepare above documents and their metadata\n",
    "The prepare_docs function below processes a list of PDF documents by extracting text from each page and organizing it into two lists: one for the text content and another for the metadata (titles). It iterates through each page of each PDF, extracts the text, and forms a title using the PDF name and page number. The function returns these two lists, making it useful for indexing and referencing the content of multiple PDFs at a page level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97e49bddd35c6d1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T08:05:12.765779Z",
     "start_time": "2024-01-07T08:05:12.763477Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_and_split_docs(pdf_directory):\n",
    "    # Load the documents\n",
    "    loader = DirectoryLoader(pdf_directory, glob=\"**/*.pdf\", show_progress=True, loader_cls=PyPDFLoader)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # Initialize a text splitter\n",
    "    splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "        chunk_size=512,  # Use the smaller chunk size here to avoid repeating splitting logic\n",
    "        chunk_overlap=256,\n",
    "        disallowed_special=(),\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"]\n",
    "    )\n",
    "\n",
    "    # Split the documents and keep metadata\n",
    "    split_docs = splitter.split_documents(documents)\n",
    "\n",
    "    print(f\"Documents are split into {len(split_docs)} passages\")\n",
    "    return split_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f576d0",
   "metadata": {},
   "source": [
    "### Step 3: Ingest into Vector Database locally\n",
    "\n",
    "The `ingest_into_vectordb` function is designed for processing and indexing a collection of documents into a vector database using FAISS (Facebook AI Similarity Search) for efficient similarity searches. It operates as follows:\n",
    "\n",
    "1. **Embedding Creation**: It generates embeddings for the input documents (`split_docs`) using the Hugging Face model `'sentence-transformers/all-MiniLM-L6-v2'`. This model is specifically chosen for its efficiency in creating sentence-level embeddings and is set to run on the CPU.\n",
    "\n",
    "2. **Vector Database Indexing**: Utilizes the generated embeddings to create a FAISS vector database. FAISS is used for its ability to efficiently handle large-scale similarity searches and clustering of dense vectors.\n",
    "\n",
    "3. **Local Storage**: After creating the vector database, the function saves it locally to the path specified by `DB_FAISS_PATH`, ensuring the data can be easily accessed for future similarity searches or retrieval tasks.\n",
    "\n",
    "The primary purpose of this function is to transform textual data into a structured, searchable vector format, facilitating efficient and scalable retrieval tasks such as document similarity searches or clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba2d9675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_into_vectordb(split_docs):\n",
    "    embeddings = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2', model_kwargs={'device': 'cpu'})\n",
    "    db = FAISS.from_documents(split_docs, embeddings)\n",
    "\n",
    "    DB_FAISS_PATH = 'vectorstore/db_faiss'\n",
    "    db.save_local(DB_FAISS_PATH)\n",
    "    print(\"Documents are inserted into FAISS vectorstore\")\n",
    "    return db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7ba4cd",
   "metadata": {},
   "source": [
    "### Step 4: Set up Conversation Chain using LLM\n",
    "The `get_conversation_chain(retriever)` function creates a stateful conversational RAG system.\n",
    "\n",
    "1. It initializes the `llama3.2` model and defines two prompts:\n",
    "   - A contextualization prompt to handle the user's query in light of the chat history.\n",
    "   - A system prompt for answering concisely with 2-3 sentences based on retrieved documents.\n",
    "\n",
    "2. It builds a `history_aware_retriever` using the retriever, LLM, and the contextualization prompt to ensure responses are context-aware.\n",
    "\n",
    "3. A `question_answer_chain` is set up to respond with answers limited to 50 words.\n",
    "\n",
    "4. These components are combined into a RAG chain using `create_retrieval_chain`.\n",
    "\n",
    "5. To manage chat history across sessions, it defines `get_session_history`, which stores and retrieves message history by session ID.\n",
    "\n",
    "6. Finally, a `RunnableWithMessageHistory` integrates the RAG chain with chat history management, ensuring the bot maintains state and provides contextually relevant responses throughout the conversation.\n",
    "\n",
    "This function sets up a sophisticated conversational AI system combining the LLaMA model for language generation and a vector database for information retrieval, enhanced with a callback manager for additional processing and a conversation memory buffer for context management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "daeb1adc421d294e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:11.383224Z",
     "start_time": "2024-01-07T07:48:11.380239Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def get_conversation_chain(retriever):\n",
    "    llm = Ollama(model=\"llama3.2\")\n",
    "    contextualize_q_system_prompt = (\n",
    "        \"Given the chat history and the latest user question, \"\n",
    "        \"provide a response that directly addresses the user's query based on the provided  documents. \"\n",
    "        \"Do not rephrase the question or ask follow-up questions.\"\n",
    "    )\n",
    "\n",
    "\n",
    "    contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", contextualize_q_system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    history_aware_retriever = create_history_aware_retriever(\n",
    "        llm, retriever, contextualize_q_prompt\n",
    "    )\n",
    "\n",
    "\n",
    "    ### Answer question ###\n",
    "    system_prompt = (\n",
    "        \"As a personal chat assistant, provide accurate and relevant information based on the provided document in 2-3 sentences. \"\n",
    "        \"Answe should be limited to 50 words and 2-3 sentences.  do not prompt to select answers or do not formualate a stand alone question. do not ask questions in the response. \"\n",
    "        \"{context}\"\n",
    "    )\n",
    "\n",
    "    qa_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            MessagesPlaceholder(\"chat_history\"),\n",
    "            (\"human\", \"{input}\"),\n",
    "        ]\n",
    "    )\n",
    "    question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "    rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "\n",
    "    ### Statefully manage chat history ###\n",
    "    store = {}\n",
    "\n",
    "\n",
    "    def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = ChatMessageHistory()\n",
    "        return store[session_id]\n",
    "\n",
    "\n",
    "    conversational_rag_chain = RunnableWithMessageHistory(\n",
    "        rag_chain,\n",
    "        get_session_history,\n",
    "        input_messages_key=\"input\",\n",
    "        history_messages_key=\"chat_history\",\n",
    "        output_messages_key=\"answer\",\n",
    "    )\n",
    "    return conversational_rag_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e69dfd",
   "metadata": {},
   "source": [
    "### Step 5: Calculate Document Similarity in the LLMs Response\n",
    "The `calculate_similarity_score` function computes the cosine similarity between a given answer and a list of context documents using Sentence Transformers. It first encodes the answer and context documents into embeddings. Then, it calculates the cosine similarities between the answer embedding and the context embeddings. The function returns the maximum similarity score, indicating how closely the answer relates to the most relevant context document. Scores range from 0 (no similarity) to 1 (perfect similarity), with higher scores reflecting better alignment with the context.\n",
    "\n",
    "Essentially, this function serves as a mechanism to check the alignment of the chatbot's response with the information in the source documents, ensuring the response's accuracy and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8d0cc0f87a9595c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T07:48:35.893137Z",
     "start_time": "2024-01-07T07:48:35.887077Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def calculate_similarity_score(answer: str, context_docs: list) -> float:\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    context_docs = [doc.page_content for doc in context_docs]\n",
    "    # Encode the answer and context documents\n",
    "    answer_embedding = model.encode(answer, convert_to_tensor=True)\n",
    "    context_embeddings = model.encode(context_docs, convert_to_tensor=True)\n",
    "\n",
    "    # Calculate cosine similarities\n",
    "    similarities = util.pytorch_cos_sim(answer_embedding, context_embeddings)\n",
    "\n",
    "    # Return the maximum similarity score from the context documents\n",
    "    max_score = similarities.max().item() \n",
    "    return max_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f38df5c",
   "metadata": {},
   "source": [
    "Now that we have crafted all the necessary functions, it's time to put them into action and test their functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e435e003cfe91c1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:24:11.041141Z",
     "start_time": "2024-01-07T10:24:10.938343Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents are split into 3 passages\n"
     ]
    }
   ],
   "source": [
    "pdf_directory=\"data_directory\"\n",
    "split_docs=prepare_and_split_docs(pdf_directory)\n",
    "vector_db= ingest_into_vectordb(split_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "62e99300",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run 12c63dea-4e8b-4270-888f-677896102224 not found for run b59d81db-dca9-4a57-9c57-80f9fbc0ed09. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vikram Bhat is a data scientist with extensive expertise in data science, analytics, and machine learning. He has worked in various roles, including Data Scientist at IBM, Data Analyst at Voxpro Groups, and BI Developer at Cognizant Technology Solutions, utilizing skills such as Python, R, SQL, and AWS Sagemaker to analyze problems, build and deploy machine learning models, and create data visualizations.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "retriever =vector_db.as_retriever()\n",
    "conversational_rag_chain=get_conversation_chain(retriever)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fa566cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Similarity Score: 0.49\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df276c10",
   "metadata": {},
   "source": [
    "### Ask your Question\n",
    "\n",
    "We created a conversational chain and now ready to chat with your own data. \n",
    "\n",
    "\n",
    "### Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e0c000474595b40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-01-07T10:44:54.014449Z",
     "start_time": "2024-01-07T10:44:50.322823Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Vikram Bhat is a data scientist with expertise in data science and analytics, machine learning, and data visualization. He has experience working in various industries, including finance and healthcare, and has developed end-to-end pipelines for building machine learning models using Watson Studio and AWS Sagemaker.Q:  who is vikram bhat?\n",
      "A:   Vikram Bhat is a data scientist with expertise in data science and analytics, machine learning, and data visualization. He has experience working in various industries, including finance and healthcare, and has developed end-to-end pipelines for building machine learning models using Watson Studio and AWS Sagemaker.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     429.30 ms\n",
      "llama_print_timings:      sample time =       5.85 ms /    68 runs   (    0.09 ms per token, 11615.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   68382.68 ms /  1435 tokens (   47.65 ms per token,    20.98 tokens per second)\n",
      "llama_print_timings:        eval time =    4554.99 ms /    67 runs   (   67.98 ms per token,    14.71 tokens per second)\n",
      "llama_print_timings:       total time =   73274.66 ms /  1502 tokens\n"
     ]
    }
   ],
   "source": [
    "\n",
    "qa1=conversational_rag_chain.invoke(\n",
    "    {\"input\": \"who is vikram bhat?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "print(qa1[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdf2f4",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "answer = qa1[\"answer\"]\n",
    "context_docs = qa1[\"context\"]\n",
    "similarity_score = calculate_similarity_score(answer, context_docs)\n",
    "\n",
    "print(\"Context Similarity Score:\", round(similarity_score,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b0d849",
   "metadata": {},
   "source": [
    "We have now received an answer for a provided question. We can also view the conversation history and source documents in the response.\n",
    "\n",
    "\n",
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "269d50b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parent run e6355841-0c20-4804-b045-67e04e91d182 not found for run 88f984a5-9fb8-49f6-a08f-a9a4e0b3f007. Treating as a root run.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vikram Bhat graduated from University College Cork, Cork with an MSc in Data Science and Analytics.\n"
     ]
    }
   ],
   "source": [
    "user_question = \"where did he graduate?\"\n",
    "\n",
    "qa2=conversational_rag_chain.invoke(\n",
    "    {\"input\": user_question},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },  # constructs a key \"abc123\" in `store`.\n",
    ")\n",
    "print(qa1[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
